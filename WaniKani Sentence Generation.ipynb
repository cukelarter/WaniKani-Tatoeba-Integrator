{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tatoeba platform will be used to generate many japanese sentences that can be translated to English. WaniKani platform will be used to filter out sentences featuring kanji based on user preferences. Both platforms use RESTful APIs to allow for data retreival.\n",
    "\n",
    "https://sakubun.xyz/known_kanji has a very good version of this desgined around the same purpose. I think the project could benefit in a more user-friendly design, rather than multiple steps in order to extract sentences. The interest lies more in developing middleware that can be easily shifted to apply to different data formats from different APIs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Interacting with WaniKani API\n",
    "We have to first interact with the WaniKani API and extract out user data. We will use the Python requests package in order to explore the correct commands for the context. Later versions should use a code base more optimized for the context. We will use requests using my personal authorization. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import json\n",
    "headers = {\n",
    "    'Authorization': 'Bearer d51a2fb4-201e-41db-ab76-0bc73d996561',\n",
    "    'Wanikani-Revision': '20170710',\n",
    "    'If-Modified-Since': 'Fri, 11 Nov 2011 11:11:11 GMT',\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Singular Resources\n",
    "The following are **singular resources**:\n",
    "* assignment\n",
    "* kanji\n",
    "* level_progression\n",
    "* radical\n",
    "* reset\n",
    "* review_statistic\n",
    "* review\n",
    "* spaced_repetition_system\n",
    "* study_material\n",
    "* user\n",
    "* vocabulary\n",
    "\n",
    "Specific subject IDs correspond to different resources. We are interested in radicals, kanji, and vocabulary. With the ID, we can retreive the resource and deserialize it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Resources by ID.\n",
    "resource='subjects'\n",
    "resource_id='/2505'\n",
    "base_url='https://api.wanikani.com/v2/'\n",
    "endpt=f'{base_url}{resource}{resource_id}'\n",
    "\n",
    "response = requests.get(endpt, headers=headers)\n",
    "if response.status_code == 200:\n",
    "    json = response.json()\n",
    "else:\n",
    "    raise Exception(f'{response} with endpoint {endpt}')\n",
    "resource=json['data']['characters']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'ふじ山'"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "resource"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Retreiving Subjects from Collections\n",
    "\n",
    "We can also extract **collections** of resources. The main source should come from filtered assignments based on criteria set by the user. Pagination and optimization with caching comes later. Retreive the collection and serialize it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filtered assignments.\n",
    "base_url='https://api.wanikani.com/v2/'\n",
    "resource='assignments'\n",
    "resource_id='?subject_types=vocabulary'\n",
    "endpt=f'{base_url}{resource}{resource_id}'\n",
    "\n",
    "response = requests.get(endpt, headers=headers)\n",
    "if response.status_code == 200:\n",
    "    json = response.json()\n",
    "else:\n",
    "    raise Exception(f'{response} with endpoint {endpt}')\n",
    "    \n",
    "assignments=json['data']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Extract the IDs of the assigned subjects from the serialized list of assignments. We can assume there are no duplicates returned from assignments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "subject_ids=[assignments[ii]['data']['subject_id'] for ii in range(len(assignments))]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "New request, passing in subject IDs, to retreive specific vocab information. This may take some time, depending on how many IDs are passed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_url='https://api.wanikani.com/v2/'\n",
    "resource='subjects'\n",
    "resource_id=f'?ids={\",\".join(map(str,subject_ids))}'\n",
    "\n",
    "endpt=f'{base_url}{resource}{resource_id}'\n",
    "response = requests.get(endpt, headers=headers)\n",
    "if response.status_code == 200:\n",
    "    json = response.json()\n",
    "else:\n",
    "    raise Exception(f'{response} with endpoint {endpt}')\n",
    "    \n",
    "subjects=json['data']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'一'"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "subjects[0]['data']['characters']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have now extracted a list of vocabulary words based on assignments on WaniKani. We can pull specific characters out and use them to search other websites for sentences containing those characters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Interacting with Tatoeba\n",
    "In the interests of dynamic programming, we should be interacting with tatoeba when we need to generate sentences from assignments. This would also benefit from cacheing and modification datetime filters. Correctness is limited by what is provided on the site, we can use other checks such contributions by native speakers. We sort by fewest words to decrease likelihood of irrelevant or unlearned vocabulary within sentence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "page=1\n",
    "query='\"一つ\"' # quotes provide explicit returns\n",
    "sort='words' # fewest words, can also do relevance\n",
    "endpt=f'https://tatoeba.org/eng/api_v0/search?from=jpn&has_audio=&native=yes&orphans=no&query={query}&sort={sort}&sort_reverse=&tags=&to=eng&trans_filter=limit&trans_has_audio=&trans_link=&trans_orphan=&trans_to=eng&trans_unapproved=&trans_user=&unapproved=no&user=&page={page}'\n",
    "response = requests.get(endpt)\n",
    "if response.status_code == 200:\n",
    "    json = response.json()\n",
    "else:\n",
    "    raise Exception(f'{response} with endpoint {endpt}')\n",
    "\n",
    "sentences=json['results']\n",
    "paging=json['paging']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With sentences retreived, we want to check the results. We can filter out grammatical syntax and sentences that contain subjects that have not been learned. This is a time to pause and reflect on how best to handle mass user inputs, where many vocab will be queried at once and redundant sentences are possible. This may require a dedicated database for the sentences in order to improve efficiency.\n",
    "\n",
    "A more efficient query would involve stripping all sentences of grammatical syntax and searching through all relevant sentences to see which ones ONLY contain subjects within user's assignments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'一つは青。'"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentences[0]['text']"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
