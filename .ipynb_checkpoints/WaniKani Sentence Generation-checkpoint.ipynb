{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tatoeba platform will be used to generate many japanese sentences that can be translated to English. WaniKani platform will be used to filter out sentences featuring kanji based on user preferences. Both platforms use RESTful APIs to allow for data retreival.\n",
    "\n",
    "https://sakubun.xyz/known_kanji has a very good version of this desgined around the same purpose. I think the project could benefit in a more user-friendly design, rather than multiple steps in order to extract sentences. The interest lies more in developing middleware that can be easily shifted to apply to different data formats from different APIs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Interacting with WaniKani API\n",
    "We have to first interact with the WaniKani API and extract out user data. We will use the Python requests package in order to explore the correct commands for the context. Later versions should use a code base more optimized for the context. We will use requests using my personal authorization. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import json\n",
    "# Import api key from file\n",
    "with open('api_key.txt') as f:\n",
    "    headers = {\n",
    "        'Authorization': f'Bearer {f.read()}',\n",
    "        'Wanikani-Revision': '20170710',\n",
    "        'If-Modified-Since': 'Fri, 11 Nov 2011 11:11:11 GMT',\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Singular Resources\n",
    "The following are **singular resources**:\n",
    "* assignment\n",
    "* kanji\n",
    "* level_progression\n",
    "* radical\n",
    "* reset\n",
    "* review_statistic\n",
    "* review\n",
    "* spaced_repetition_system\n",
    "* study_material\n",
    "* user\n",
    "* vocabulary\n",
    "\n",
    "Specific subject IDs correspond to different resources. We are interested in radicals, kanji, and vocabulary. With the ID, we can retreive the resource and deserialize it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Resources by ID.\n",
    "resource='subjects'\n",
    "resource_id='/2505'\n",
    "base_url='https://api.wanikani.com/v2/'\n",
    "endpt=f'{base_url}{resource}{resource_id}'\n",
    "\n",
    "response = requests.get(endpt, headers=headers)\n",
    "if response.status_code == 200:\n",
    "    json = response.json()\n",
    "else:\n",
    "    raise Exception(f'{response} with endpoint {endpt}')\n",
    "resource=json['data']['characters']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'ふじ山'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "resource"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Retreiving Subjects from Collections\n",
    "\n",
    "We can also extract **collections** of resources. The main source should come from filtered assignments based on criteria set by the user. Pagination and optimization with caching comes later. Retreive the collection and serialize it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filtered assignments.\n",
    "base_url='https://api.wanikani.com/v2/'\n",
    "resource='assignments'\n",
    "resource_id='?subject_types=vocabulary'\n",
    "endpt=f'{base_url}{resource}{resource_id}'\n",
    "\n",
    "response = requests.get(endpt, headers=headers)\n",
    "if response.status_code == 200:\n",
    "    json = response.json()\n",
    "else:\n",
    "    raise Exception(f'{response} with endpoint {endpt}')\n",
    "    \n",
    "assignments=json['data']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Extract the IDs of the assigned subjects from the serialized list of assignments. We can assume there are no duplicates returned from assignments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "subject_ids=[assignments[ii]['data']['subject_id'] for ii in range(len(assignments))]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "New request, passing in subject IDs, to retreive specific vocab information. This may take some time, depending on how many IDs are passed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_url='https://api.wanikani.com/v2/'\n",
    "resource='subjects'\n",
    "resource_id=f'?ids={\",\".join(map(str,subject_ids))}'\n",
    "\n",
    "endpt=f'{base_url}{resource}{resource_id}'\n",
    "response = requests.get(endpt, headers=headers)\n",
    "if response.status_code == 200:\n",
    "    json = response.json()\n",
    "else:\n",
    "    raise Exception(f'{response} with endpoint {endpt}')\n",
    "    \n",
    "subjects=json['data']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have now extracted a list of vocabulary words based on assignments on WaniKani. We can pull specific characters out and use them to search other websites for sentences containing those characters\n",
    "\n",
    "### Pre-processing by Parts of Speech\n",
    "Each vocabulary word can be defined by its part of speech, each of which have different rules for downstream processes. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(30, '口', ['noun']),\n",
       " (31, '入り口', ['noun']),\n",
       " (32, '大きい', ['い adjective']),\n",
       " (33, '大きさ', ['noun']),\n",
       " (34, '大した', ['adjective']),\n",
       " (35, '大人', ['noun', 'な adjective', 'の adjective']),\n",
       " (36, '女', ['noun']),\n",
       " (37, '山', ['noun']),\n",
       " (38, 'ふじ山', ['proper noun']),\n",
       " (39, '川', ['noun'])]"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[(i,s['data']['characters'],s['data']['parts_of_speech']) for i,s in enumerate(subjects)][30:40]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* **Verbs** - Destructive modification to sentence, need to reduce to root without destroying meaning. For now we will simply remove the last radical in the vocabulary word, but this is by no means robust.\n",
    "    * Transitive, intransitive\n",
    "    * Godan, ichidan\n",
    "* **Adjectives** - Additive change sometimes.\n",
    "* **Nouns** - No major modifications.\n",
    "* **Adverbs** - No major modifications."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Interacting with Tatoeba\n",
    "In the interests of development speed, we should be interacting with tatoeba when we need to generate sentences from assignments. Deployment should host database for sentences. This would also benefit from cacheing and modification datetime filters. Correctness is limited by what is provided on the site, we can use other checks such contributions by native speakers. We sort by fewest words to decrease likelihood of irrelevant or unlearned vocabulary within sentence.\n",
    "\n",
    "Query for pagination data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [],
   "source": [
    "query='\"字\"' # inner quotes provide explicit returns\n",
    "sort='words' # fewest words, can also do relevance\n",
    "endpt=f'https://tatoeba.org/eng/api_v0/search?from=jpn&has_audio=&native=yes&orphans=no&query={query}&sort={sort}&sort_reverse=&tags=&to=eng&trans_filter=limit&trans_has_audio=&trans_link=&trans_orphan=&trans_to=eng&trans_unapproved=&trans_user=&unapproved=no&user=&'\n",
    "response = requests.get(endpt)\n",
    "if response.status_code == 200:\n",
    "    json = response.json()\n",
    "else:\n",
    "    raise Exception(f'{response} with endpoint {endpt}')\n",
    "paging=json['paging']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After first request we can loop through pagination to retreive all sentences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm.notebook import tqdm # progress bar package import\n",
    "import time # for requests delay"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "663020fa840347828ca1083be508d01e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Extracting...:   0%|          | 0/18 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished extracting 171 sentences matching \"字\".\n"
     ]
    }
   ],
   "source": [
    "sentences=[]\n",
    "nPages=paging['Sentences']['pageCount']\n",
    "for page in tqdm(range(1,nPages+1),desc=\"Extracting...\"):\n",
    "    endpt=f'https://tatoeba.org/eng/api_v0/search?from=jpn&has_audio=&native=yes&orphans=no&query={query}&sort={sort}&sort_reverse=&tags=&to=eng&trans_filter=limit&trans_has_audio=&trans_link=&trans_orphan=&trans_to=eng&trans_unapproved=&trans_user=&unapproved=no&user=&page={page}'\n",
    "    response = requests.get(endpt)\n",
    "    if response.status_code == 200:\n",
    "        json = response.json()\n",
    "    else:\n",
    "        raise Exception(f'{response} with endpoint {endpt}')\n",
    "    sentences.extend(json['results'])\n",
    "    # Pause so requests doesn't overload. Most wait time is server-side, optimize by linking database during deployment.\n",
    "    time.sleep(0.25)\n",
    "print(f'Finished extracting {len(sentences)} sentences matching {query}.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With sentences retreived, we want to check the results. We can filter out grammatical syntax and sentences that contain subjects that have not been learned. This is a time to pause and reflect on how best to handle mass user inputs, where many vocab will be queried at once and redundant sentences are possible. This may require a dedicated database for the sentences in order to improve efficiency.\n",
    "\n",
    "A more efficient query would involve stripping all sentences of grammatical syntax and searching through all relevant sentences to see which ones ONLY contain subjects within user's assignments.\n",
    "\n",
    "### Strip grammatical syntax\n",
    "Strip select characters that are used for grammar."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "metadata": {},
   "outputs": [],
   "source": [
    "def linestrip(line):\n",
    "    chars_grammar=''\n",
    "    translation_table = dict.fromkeys(map(ord, chars_grammar), None)\n",
    "    unicode_line = unicode_line.translate(translation_table)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Scoring Sentences Based on Vocabulary Relevance\n",
    "Compare sample of sentences to sample of vocabulary subjects being tested, wich higher score denoting higher composition of sampled subjects and therefore higher likelihood that participant can guess the sentence meaning. This works around some of the particle and conjugation issue if we remember to drop the endings where needed (some verb types)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['女', '手', '上手', '字']"
      ]
     },
     "execution_count": 186,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test=[[v for v in vocab if (v in s['text'])] for s in sentences][4]\n",
    "test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab=[s['data']['characters'] for i,s in enumerate(subjects)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Start by scoring each sentence on how many times an assigned vocab word appears. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "metadata": {},
   "outputs": [],
   "source": [
    "scoring=[]\n",
    "for s in sentences:\n",
    "    matches=[v for v in vocab if (v in s['text'])]\n",
    "    # check for duplicates by splitting vocab into component parts\n",
    "    # for now only worry about two-kanji vocab\n",
    "    # this code is inelegant and slow pls phase out at some point\n",
    "    for v in matches:\n",
    "        if len(v)>1:\n",
    "            for char in v:\n",
    "                if char in matches:\n",
    "                    matches.remove(char)\n",
    "    scoring.extend([s['text'],matches])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['彼は字が下手だ。', ['下手', '字'], '渡辺が名字です。', ['名字'], '何の略字ですか。']"
      ]
     },
     "execution_count": 199,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scoring[0:5]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
